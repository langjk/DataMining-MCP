# gpt_dispatcher.py
# å·¥å…·è°ƒç”¨å“åº”è°ƒåº¦å™¨ï¼Œä»…è´Ÿè´£è§£æ GPT æŒ‡ä»¤å¹¶æ‰§è¡Œå¯¹åº”å·¥å…·

import json
from scan import scan_directory
from csv_reader import read_csv_clean
from cache_utils import save_cache, load_cache
from data_analyzer import DataAnalyzer

def dispatch_gpt_response(gpt_reply: str):
    """
    è§£æ GPT è¿”å›çš„ JSON æŒ‡ä»¤ï¼Œå¹¶è°ƒç”¨ç›¸åº”å·¥å…·ã€‚
    è¿”å›ç»Ÿä¸€ç»“æ„ï¼š{"type": "tool_result" / "noop" / "error", "data": object}
    """
    print(f"ğŸ”§ [DEBUG] dispatch_gpt_response æ”¶åˆ°GPTå›å¤: {gpt_reply[:200]}...")
    
    try:
        data = json.loads(gpt_reply)
        if not isinstance(data, dict) or "tool" not in data:
            print("ğŸ”§ [DEBUG] GPTè¿”å›æ ¼å¼é”™è¯¯")
            return {"type": "error", "content": "GPT è¿”å›æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ tool å­—æ®µã€‚"}

        tool = data["tool"]
        args = data.get("args", {})
        
        print(f"ğŸ”§ [DEBUG] è°ƒç”¨å·¥å…·: {tool}")
        print(f"ğŸ”§ [DEBUG] å·¥å…·å‚æ•°: {args}")

        if tool == "scan":
            result = scan_directory(args.get("path", "./data"))
            return {
                "type": "tool_result",
                "tool": "scan",
                "data": result
            }

        elif tool == "csv_reader":
            df, meta = read_csv_clean(
                path=args["path"],
                nrows=args.get("num_rows"),
                remove_header_repeats=True
            )
            return {
                "type": "tool_result",
                "tool": "csv_reader",
                "data": {
                    "fields": df.columns.tolist(),
                    "preview": df.head().to_dict(orient="records"),
                    "metadata": meta
                }
            }

        elif tool == "cache":
            mode = args.get("mode")
            key = args.get("key")
            if mode == "write":
                content = args.get("content", {})
                save_cache(key, content)
                return {
                    "type": "tool_result",
                    "tool": "cache",
                    "data": {"status": "saved", "key": key}
                }
            elif mode == "read":
                content = load_cache(key)
                return {
                    "type": "tool_result",
                    "tool": "cache",
                    "data": {"status": "loaded", "key": key, "content": content}
                }
            else:
                return {"type": "error", "content": f"æ— æ•ˆç¼“å­˜æ¨¡å¼ï¼š{mode}"}

        elif tool == "data_analysis":
            analyzer = DataAnalyzer()
            analysis_type = args.get("analysis_type", "comprehensive")
            file_path = args.get("file_path")
            columns = args.get("columns")
            time_column = args.get("time_column")
            
            if not file_path:
                return {"type": "error", "content": "data_analysis å·¥å…·éœ€è¦ file_path å‚æ•°"}
            
            try:
                # è¯»å–æ•°æ®
                df, meta = read_csv_clean(file_path)
                
                # æ ¹æ®åˆ†æç±»å‹è°ƒç”¨ç›¸åº”æ–¹æ³•
                if analysis_type == "comprehensive":
                    result = analyzer.comprehensive_analysis(df, columns, time_column)
                    # ç²¾ç®€ç»¼åˆåˆ†æç»“æœ
                    simplified_result = _simplify_comprehensive_analysis(result)
                elif analysis_type == "statistics":
                    result = analyzer.basic_statistics(df, columns)
                    simplified_result = _simplify_statistics(result)
                elif analysis_type == "stability":
                    result = analyzer.stability_analysis(df, columns)
                    simplified_result = _simplify_stability_analysis(result)
                elif analysis_type == "trend":
                    result = analyzer.trend_analysis(df, columns, time_column)
                    simplified_result = _simplify_trend_analysis(result)
                elif analysis_type == "outlier":
                    result = analyzer.outlier_detection(df, columns)
                    simplified_result = _simplify_outlier_detection(result)
                else:
                    return {"type": "error", "content": f"æœªè¯†åˆ«çš„åˆ†æç±»å‹ï¼š{analysis_type}"}
                
                return {
                    "type": "tool_result",
                    "tool": "data_analysis",
                    "data": {
                        "analysis_type": analysis_type,
                        "file_path": file_path,
                        "file_metadata": meta,
                        "analysis_results": simplified_result
                    }
                }
            except Exception as e:
                return {"type": "error", "content": f"æ•°æ®åˆ†æå¤±è´¥ï¼š{str(e)}"}

        elif tool == "batch_comparison":
            analyzer = DataAnalyzer()
            batch1_path = args.get("batch1_path")
            batch2_path = args.get("batch2_path")
            batch1_name = args.get("batch1_name", "æ‰¹æ¬¡1")
            batch2_name = args.get("batch2_name", "æ‰¹æ¬¡2")
            columns = args.get("columns")
            
            if not batch1_path or not batch2_path:
                return {"type": "error", "content": "batch_comparison å·¥å…·éœ€è¦ batch1_path å’Œ batch2_path å‚æ•°"}
            
            try:
                # è¯»å–ä¸¤ä¸ªæ‰¹æ¬¡çš„æ•°æ®
                df1, meta1 = read_csv_clean(batch1_path)
                df2, meta2 = read_csv_clean(batch2_path)
                
                # æ‰§è¡Œæ‰¹æ¬¡å¯¹æ¯”åˆ†æ
                result = analyzer.batch_comparison(df1, df2, columns, batch1_name, batch2_name)
                
                # ç²¾ç®€æ‰¹æ¬¡å¯¹æ¯”ç»“æœç”¨äºGPTå¤„ç†
                simplified_result = _simplify_batch_comparison(result)
                
                return {
                    "type": "tool_result",
                    "tool": "batch_comparison",
                    "data": {
                        "batch1_metadata": {"filename": meta1.get("filename", ""), "rows": meta1.get("rows", 0)},
                        "batch2_metadata": {"filename": meta2.get("filename", ""), "rows": meta2.get("rows", 0)},
                        "analysis_results": result,  # å®Œæ•´ç»“æœç”¨äºå³ä¾§é¢æ¿æ˜¾ç¤º
                        "comparison_results": simplified_result  # ç²¾ç®€ç»“æœç”¨äºGPTåˆ†æ
                    }
                }
            except Exception as e:
                return {"type": "error", "content": f"æ‰¹æ¬¡å¯¹æ¯”åˆ†æå¤±è´¥ï¼š{str(e)}"}

        elif tool == "multi_batch_analysis":
            analyzer = DataAnalyzer()
            batch_paths = args.get("batch_paths", [])
            batch_names = args.get("batch_names", [])
            analysis_type = args.get("analysis_type", "stability_trend")  # stability_trend, outlier_detection, comprehensive
            columns = args.get("columns")
            time_column = args.get("time_column")
            
            if not batch_paths or len(batch_paths) < 2:
                return {"type": "error", "content": "multi_batch_analysis å·¥å…·éœ€è¦è‡³å°‘2ä¸ªæ‰¹æ¬¡è·¯å¾„"}
            
            try:
                # è¯»å–æ‰€æœ‰æ‰¹æ¬¡æ•°æ®
                batch_data = []
                batch_metadata = []
                
                for i, path in enumerate(batch_paths):
                    df, meta = read_csv_clean(path)
                    batch_name = batch_names[i] if i < len(batch_names) else f"æ‰¹æ¬¡{i+1}"
                    batch_data.append((df, batch_name))
                    batch_metadata.append({
                        "filename": meta.get("filename", ""),
                        "rows": meta.get("rows", 0),
                        "batch_name": batch_name
                    })
                
                # æ‰§è¡Œå¤šæ‰¹æ¬¡åˆ†æ
                if analysis_type == "stability_trend":
                    # åˆ†ææ‰€æœ‰æ‰¹æ¬¡çš„ç¨³å®šæ€§å˜åŒ–è¶‹åŠ¿ï¼ˆä½¿ç”¨å…¨é¢å¯¹æ¯”åˆ†æï¼‰
                    result = _analyze_multi_batch_comprehensive_comparison(analyzer, batch_data, columns)
                elif analysis_type == "outlier_detection":
                    # æ‰¾å‡ºå¼‚å¸¸æ‰¹æ¬¡
                    result = _analyze_multi_batch_outliers(analyzer, batch_data, columns)
                elif analysis_type == "comprehensive":
                    # å…¨é¢åˆ†ææ‰€æœ‰æ‰¹æ¬¡ï¼ˆåŒ…æ‹¬æ‰€æœ‰ç»Ÿè®¡æŒ‡æ ‡å¯¹æ¯”ï¼‰
                    result = _analyze_multi_batch_comprehensive_comparison(analyzer, batch_data, columns)
                else:
                    return {"type": "error", "content": f"æœªè¯†åˆ«çš„å¤šæ‰¹æ¬¡åˆ†æç±»å‹ï¼š{analysis_type}"}
                
                return {
                    "type": "tool_result",
                    "tool": "multi_batch_analysis",
                    "data": {
                        "analysis_type": analysis_type,
                        "batch_metadata": batch_metadata,
                        "analysis_results": result,
                        "total_batches": len(batch_paths)
                    }
                }
            except Exception as e:
                return {"type": "error", "content": f"å¤šæ‰¹æ¬¡åˆ†æå¤±è´¥ï¼š{str(e)}"}

        elif tool == "time_series_analysis":
            print("ğŸ”§ [DEBUG] å¼€å§‹æ—¶é—´åºåˆ—åˆ†æ")
            analyzer = DataAnalyzer()
            file_path = args.get("file_path")
            columns = args.get("columns")
            time_column = args.get("time_column")
            
            print(f"ğŸ”§ [DEBUG] æ–‡ä»¶è·¯å¾„: {file_path}")
            print(f"ğŸ”§ [DEBUG] æŒ‡å®šåˆ—: {columns}")
            print(f"ğŸ”§ [DEBUG] æ—¶é—´åˆ—: {time_column}")
            
            if not file_path:
                return {"type": "error", "content": "time_series_analysis å·¥å…·éœ€è¦ file_path å‚æ•°"}
            
            try:
                # è¯»å–æ•°æ®
                print(f"ğŸ”§ [DEBUG] æ­£åœ¨è¯»å–æ–‡ä»¶: {file_path}")
                df, meta = read_csv_clean(file_path)
                print(f"ğŸ”§ [DEBUG] æ•°æ®è¯»å–æˆåŠŸï¼Œå½¢çŠ¶: {df.shape}")
                
                # æ—¶é—´åºåˆ—åˆ†æ
                print("ğŸ”§ [DEBUG] å¼€å§‹æ‰§è¡Œæ—¶é—´åºåˆ—åˆ†æ")
                result = _analyze_time_series(analyzer, df, columns, time_column)
                print(f"ğŸ”§ [DEBUG] æ—¶é—´åºåˆ—åˆ†æå®Œæˆï¼Œåˆ†æç»“æœæ•°é‡: {len(result.get('series_analysis', {}))}")
                
                print("ğŸ”§ [DEBUG] è¿”å›æ—¶é—´åºåˆ—åˆ†æç»“æœ")
                return {
                    "type": "tool_result",
                    "tool": "time_series_analysis",
                    "data": {
                        "file_path": file_path,
                        "file_metadata": meta,
                        "analysis_results": result
                    }
                }
            except Exception as e:
                print(f"ğŸ”§ [DEBUG] æ—¶é—´åºåˆ—åˆ†æå¼‚å¸¸: {str(e)}")
                return {"type": "error", "content": f"æ—¶é—´åºåˆ—åˆ†æå¤±è´¥ï¼š{str(e)}"}

        elif tool == "noop":
            return {"type": "noop", "content": data.get("reply", "")}

        return {"type": "error", "content": f"æœªè¯†åˆ«çš„å·¥å…·ç±»å‹ï¼š{tool}"}

    except Exception as e:
        return {"type": "error", "content": f"å·¥å…·è°ƒç”¨å¼‚å¸¸ï¼š{str(e)}"}

def _simplify_comprehensive_analysis(result):
    """ç²¾ç®€ç»¼åˆåˆ†æç»“æœï¼Œä¿ç•™å…³é”®ä¿¡æ¯"""
    simplified = {
        "data_quality": result.get("data_quality", {}),
        "summary": result.get("summary", {}),
        "key_metrics_statistics": {},
        "key_metrics_stability": {},
        "key_metrics_trends": {},
        "outlier_summary": {}
    }
    
    # é€‰æ‹©å‰5ä¸ªå…³é”®æŒ‡æ ‡çš„ç»Ÿè®¡ä¿¡æ¯
    stats = result.get("basic_statistics", {})
    count = 0
    for metric, data in stats.items():
        if count >= 5:
            break
        simplified["key_metrics_statistics"][metric] = {
            "mean": round(data.get("mean", 0), 4),
            "std": round(data.get("std", 0), 4),
            "cv": round(data.get("cv", 0), 4),
            "range": [data.get("min", 0), data.get("max", 0)]
        }
        count += 1
    
    # é€‰æ‹©å‰5ä¸ªå…³é”®æŒ‡æ ‡çš„ç¨³å®šæ€§ä¿¡æ¯
    stability = result.get("stability_analysis", {})
    count = 0
    for metric, data in stability.items():
        if count >= 5:
            break
        simplified["key_metrics_stability"][metric] = {
            "stability_rating": data.get("stability_rating", ""),
            "stability_index": round(data.get("stability_index", 0), 4),
            "max_change_rate": round(data.get("max_change_rate", 0), 4)
        }
        count += 1
    
    # é€‰æ‹©å‰5ä¸ªå…³é”®æŒ‡æ ‡çš„è¶‹åŠ¿ä¿¡æ¯
    trends = result.get("trend_analysis", {})
    count = 0
    for metric, data in trends.items():
        if count >= 5:
            break
        if "error" not in data:
            simplified["key_metrics_trends"][metric] = {
                "trend_direction": data.get("trend_direction", ""),
                "trend_strength": data.get("trend_strength", ""),
                "is_significant": data.get("is_significant", False),
                "relative_change": round(data.get("relative_change", 0), 2)
            }
        count += 1
    
    # å¼‚å¸¸æ£€æµ‹æ€»ç»“
    outliers = result.get("outlier_detection", {})
    total_outliers = 0
    outlier_metrics = []
    for metric, data in outliers.items():
        outlier_count = data.get("outlier_count", 0)
        if outlier_count > 0:
            total_outliers += outlier_count
            outlier_metrics.append({
                "metric": metric,
                "count": outlier_count,
                "percentage": round(data.get("outlier_percentage", 0), 2)
            })
    
    simplified["outlier_summary"] = {
        "total_outliers": total_outliers,
        "affected_metrics": len(outlier_metrics),
        "top_outlier_metrics": outlier_metrics[:3]  # åªæ˜¾ç¤ºå‰3ä¸ª
    }
    
    return simplified

def _simplify_statistics(result):
    """ç²¾ç®€ç»Ÿè®¡åˆ†æç»“æœ"""
    simplified = {}
    for metric, data in result.items():
        simplified[metric] = {
            "mean": round(data.get("mean", 0), 4),
            "std": round(data.get("std", 0), 4),
            "cv": round(data.get("cv", 0), 4),
            "range": [data.get("min", 0), data.get("max", 0)]
        }
    return simplified

def _simplify_stability_analysis(result):
    """ç²¾ç®€ç¨³å®šæ€§åˆ†æç»“æœ"""
    simplified = {}
    for metric, data in result.items():
        simplified[metric] = {
            "stability_rating": data.get("stability_rating", ""),
            "stability_index": round(data.get("stability_index", 0), 4),
            "max_change_rate": round(data.get("max_change_rate", 0), 4)
        }
    return simplified

def _simplify_trend_analysis(result):
    """ç²¾ç®€è¶‹åŠ¿åˆ†æç»“æœ"""
    simplified = {}
    for metric, data in result.items():
        if "error" not in data:
            simplified[metric] = {
                "trend_direction": data.get("trend_direction", ""),
                "trend_strength": data.get("trend_strength", ""),
                "is_significant": data.get("is_significant", False),
                "correlation": round(data.get("correlation", 0), 4),
                "relative_change": round(data.get("relative_change", 0), 2)
            }
        else:
            simplified[metric] = {"error": data.get("error", "")}
    return simplified

def _simplify_outlier_detection(result):
    """ç²¾ç®€å¼‚å¸¸æ£€æµ‹ç»“æœ"""
    simplified = {}
    for metric, data in result.items():
        simplified[metric] = {
            "outlier_count": data.get("outlier_count", 0),
            "outlier_percentage": round(data.get("outlier_percentage", 0), 2),
            "has_outliers": data.get("outlier_count", 0) > 0
        }
    return simplified

def _simplify_batch_comparison(result):
    """ç²¾ç®€æ‰¹æ¬¡å¯¹æ¯”ç»“æœ"""
    simplified = {
        "batch1_name": result.get("batch1_name", ""),
        "batch2_name": result.get("batch2_name", ""),
        "comparison_summary": {},
        "key_differences": []
    }
    
    comparison = result.get("comparison", {})
    
    # ç»Ÿè®¡æ”¹è¿›æƒ…å†µ
    improvement_counts = {"æ˜¾è‘—æ”¹å–„": 0, "ç¨³å®šæ€§æ”¹å–„": 0, "æ•°å€¼æ”¹å–„": 0, "éœ€è¦å…³æ³¨": 0}
    significant_diffs = 0
    key_metrics = []
    
    for metric, data in comparison.items():
        improvement = data.get("improvement", "")
        if improvement in improvement_counts:
            improvement_counts[improvement] += 1
        
        if data.get("significant_difference", False):
            significant_diffs += 1
        
        # ä¿å­˜å…³é”®å·®å¼‚ï¼ˆå‰5ä¸ªæŒ‡æ ‡ï¼‰
        if len(key_metrics) < 5:
            key_metrics.append({
                "metric": metric,
                "mean_change_percent": round(data.get("mean_change_percent", 0), 2),
                "stability_comparison": data.get("stability_comparison", ""),
                "improvement": improvement,
                "significant": data.get("significant_difference", False)
            })
    
    simplified["comparison_summary"] = {
        "total_metrics": len(comparison),
        "significant_differences": significant_diffs,
        "improvement_distribution": improvement_counts
    }
    
    simplified["key_differences"] = key_metrics
    
    return simplified

def _analyze_multi_batch_comprehensive_comparison(analyzer, batch_data, columns):
    """å…¨é¢çš„å¤šæ‰¹æ¬¡å¯¹æ¯”åˆ†æï¼ˆæ”¯æŒæ‰€æœ‰ç»Ÿè®¡æŒ‡æ ‡ï¼‰"""
    import numpy as np
    
    batch_comprehensive_data = {}
    multi_metrics_trends = {}
    
    # ä¸ºæ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œå…¨é¢åˆ†æ
    for df, batch_name in batch_data:
        # è·å–å…¨é¢çš„åˆ†æç»“æœ
        comprehensive_result = analyzer.comprehensive_analysis(df, columns, None)
        
        # æå–å„ç±»æŒ‡æ ‡
        basic_stats = comprehensive_result.get('basic_statistics', {})
        stability_analysis = comprehensive_result.get('stability_analysis', {})
        trend_analysis = comprehensive_result.get('trend_analysis', {})
        outlier_detection = comprehensive_result.get('outlier_detection', {})
        
        batch_comprehensive_data[batch_name] = {
            'basic_statistics': basic_stats,
            'stability_analysis': stability_analysis,
            'trend_analysis': trend_analysis,
            'outlier_detection': outlier_detection
        }
    
    # è·å–è¦åˆ†æçš„åˆ—
    if columns:
        target_columns = [col for col in columns if col in batch_data[0][0].columns]
    else:
        target_columns = [col for col in batch_data[0][0].columns if batch_data[0][0][col].dtype in ['int64', 'float64']][:10]
    
    # å¯¹æ¯ä¸ªæŒ‡æ ‡è¿›è¡Œå¤šæ‰¹æ¬¡å¯¹æ¯”åˆ†æ
    for column in target_columns:
        batch_names = []
        
        # æ”¶é›†å„ç±»æŒ‡æ ‡æ•°æ®
        means = []
        stds = []
        mins = []
        maxs = []
        cvs = []
        stability_indices = []
        outlier_counts = []
        trend_correlations = []
        
        for df, batch_name in batch_data:
            batch_data_item = batch_comprehensive_data[batch_name]
            
            # åŸºç¡€ç»Ÿè®¡æ•°æ®
            basic_stats = batch_data_item['basic_statistics']
            stability_data = batch_data_item['stability_analysis']
            trend_data = batch_data_item['trend_analysis']
            outlier_data = batch_data_item['outlier_detection']
            
            if column in basic_stats:
                batch_names.append(batch_name)
                means.append(basic_stats[column].get('mean', 0))
                stds.append(basic_stats[column].get('std', 0))
                mins.append(basic_stats[column].get('min', 0))
                maxs.append(basic_stats[column].get('max', 0))
                cvs.append(basic_stats[column].get('cv', 0))
                
                # ç¨³å®šæ€§æ•°æ®
                if column in stability_data:
                    stability_indices.append(stability_data[column].get('stability_index', 0))
                else:
                    stability_indices.append(0)
                
                # å¼‚å¸¸æ£€æµ‹æ•°æ®
                if column in outlier_data:
                    outlier_counts.append(outlier_data[column].get('outlier_count', 0))
                else:
                    outlier_counts.append(0)
                
                # è¶‹åŠ¿æ•°æ®
                if column in trend_data and 'error' not in trend_data[column]:
                    trend_correlations.append(trend_data[column].get('correlation', 0))
                else:
                    trend_correlations.append(0)
        
        if len(batch_names) >= 2:
            # è®¡ç®—å„æŒ‡æ ‡çš„è¶‹åŠ¿
            x = np.arange(len(batch_names))
            
            multi_metrics_trends[column] = {
                'batch_names': batch_names,
                'metrics': {
                    'mean': {
                        'values': [round(v, 4) for v in means],
                        'trend_slope': round(np.polyfit(x, means, 1)[0], 6) if len(means) > 1 else 0,
                        'trend_direction': _get_trend_direction(np.polyfit(x, means, 1)[0]) if len(means) > 1 else 'ç¨³å®š',
                        'change_percent': round(((means[-1] - means[0]) / means[0] * 100), 2) if means[0] != 0 else 0,
                        'range': round(max(means) - min(means), 4),
                        'best_batch': batch_names[np.argmax(means)] if means else '',
                        'worst_batch': batch_names[np.argmin(means)] if means else ''
                    },
                    'std': {
                        'values': [round(v, 4) for v in stds],
                        'trend_slope': round(np.polyfit(x, stds, 1)[0], 6) if len(stds) > 1 else 0,
                        'trend_direction': _get_trend_direction(np.polyfit(x, stds, 1)[0]) if len(stds) > 1 else 'ç¨³å®š',
                        'change_percent': round(((stds[-1] - stds[0]) / stds[0] * 100), 2) if stds[0] != 0 else 0,
                        'range': round(max(stds) - min(stds), 4),
                        'best_batch': batch_names[np.argmin(stds)] if stds else '',  # æ ‡å‡†å·®è¶Šå°è¶Šå¥½
                        'worst_batch': batch_names[np.argmax(stds)] if stds else ''
                    },
                    'cv': {
                        'values': [round(v, 4) for v in cvs],
                        'trend_slope': round(np.polyfit(x, cvs, 1)[0], 6) if len(cvs) > 1 else 0,
                        'trend_direction': _get_trend_direction(np.polyfit(x, cvs, 1)[0]) if len(cvs) > 1 else 'ç¨³å®š',
                        'change_percent': round(((cvs[-1] - cvs[0]) / cvs[0] * 100), 2) if cvs[0] != 0 else 0,
                        'range': round(max(cvs) - min(cvs), 4),
                        'best_batch': batch_names[np.argmin(cvs)] if cvs else '',  # å˜å¼‚ç³»æ•°è¶Šå°è¶Šå¥½
                        'worst_batch': batch_names[np.argmax(cvs)] if cvs else ''
                    },
                    'min': {
                        'values': [round(v, 4) for v in mins],
                        'trend_slope': round(np.polyfit(x, mins, 1)[0], 6) if len(mins) > 1 else 0,
                        'trend_direction': _get_trend_direction(np.polyfit(x, mins, 1)[0]) if len(mins) > 1 else 'ç¨³å®š',
                        'change_percent': round(((mins[-1] - mins[0]) / mins[0] * 100), 2) if mins[0] != 0 else 0,
                        'range': round(max(mins) - min(mins), 4)
                    },
                    'max': {
                        'values': [round(v, 4) for v in maxs],
                        'trend_slope': round(np.polyfit(x, maxs, 1)[0], 6) if len(maxs) > 1 else 0,
                        'trend_direction': _get_trend_direction(np.polyfit(x, maxs, 1)[0]) if len(maxs) > 1 else 'ç¨³å®š',
                        'change_percent': round(((maxs[-1] - maxs[0]) / maxs[0] * 100), 2) if maxs[0] != 0 else 0,
                        'range': round(max(maxs) - min(maxs), 4)
                    },
                    'stability_index': {
                        'values': [round(v, 4) for v in stability_indices],
                        'trend_slope': round(np.polyfit(x, stability_indices, 1)[0], 6) if len(stability_indices) > 1 else 0,
                        'trend_direction': _get_trend_direction(np.polyfit(x, stability_indices, 1)[0]) if len(stability_indices) > 1 else 'ç¨³å®š',
                        'change_percent': round(((stability_indices[-1] - stability_indices[0]) / stability_indices[0] * 100), 2) if stability_indices[0] != 0 else 0,
                        'range': round(max(stability_indices) - min(stability_indices), 4),
                        'best_batch': batch_names[np.argmax(stability_indices)] if stability_indices else '',
                        'worst_batch': batch_names[np.argmin(stability_indices)] if stability_indices else ''
                    },
                    'outlier_count': {
                        'values': outlier_counts,
                        'trend_slope': round(np.polyfit(x, outlier_counts, 1)[0], 6) if len(outlier_counts) > 1 else 0,
                        'trend_direction': _get_trend_direction(np.polyfit(x, outlier_counts, 1)[0]) if len(outlier_counts) > 1 else 'ç¨³å®š',
                        'change_percent': round(((outlier_counts[-1] - outlier_counts[0]) / outlier_counts[0] * 100), 2) if outlier_counts[0] != 0 else 0,
                        'range': max(outlier_counts) - min(outlier_counts) if outlier_counts else 0,
                        'best_batch': batch_names[np.argmin(outlier_counts)] if outlier_counts else '',  # å¼‚å¸¸å€¼è¶Šå°‘è¶Šå¥½
                        'worst_batch': batch_names[np.argmax(outlier_counts)] if outlier_counts else ''
                    }
                }
            }
    
    return {
        'analysis_type': 'multi_batch_comprehensive',
        'batch_comprehensive_data': batch_comprehensive_data,
        'multi_metrics_trends': multi_metrics_trends,
        'summary': {
            'total_batches': len(batch_data),
            'analyzed_metrics': len(multi_metrics_trends),
            'available_metric_types': ['mean', 'std', 'cv', 'min', 'max', 'stability_index', 'outlier_count']
        }
    }

def _get_trend_direction(slope):
    """æ ¹æ®æ–œç‡åˆ¤æ–­è¶‹åŠ¿æ–¹å‘"""
    if slope > 0.001:
        return 'ä¸Šå‡'
    elif slope < -0.001:
        return 'ä¸‹é™'
    else:
        return 'ç¨³å®š'

def _analyze_multi_batch_outliers(analyzer, batch_data, columns):
    """æ‰¾å‡ºå¼‚å¸¸æ‰¹æ¬¡"""
    import numpy as np
    from scipy import stats
    
    batch_stats = {}
    outlier_analysis = {}
    
    # è®¡ç®—æ¯ä¸ªæ‰¹æ¬¡çš„åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
    for df, batch_name in batch_data:
        stats_result = analyzer.basic_statistics(df, columns)
        batch_stats[batch_name] = stats_result
    
    # åˆ†æå“ªäº›æ‰¹æ¬¡æ˜¯å¼‚å¸¸çš„
    if columns:
        target_columns = [col for col in columns if col in batch_data[0][0].columns]
    else:
        target_columns = [col for col in batch_data[0][0].columns if batch_data[0][0][col].dtype in ['int64', 'float64']][:10]
    
    batch_outlier_scores = {batch_name: 0 for _, batch_name in batch_data}
    
    for column in target_columns:
        means = []
        stds = []
        batch_names = []
        
        for df, batch_name in batch_data:
            if column in batch_stats[batch_name]:
                mean_val = batch_stats[batch_name][column].get('mean', 0)
                std_val = batch_stats[batch_name][column].get('std', 0)
                means.append(mean_val)
                stds.append(std_val)
                batch_names.append(batch_name)
        
        if len(means) >= 3:
            # è®¡ç®—Z-scoreæ¥è¯†åˆ«å¼‚å¸¸æ‰¹æ¬¡
            mean_z_scores = np.abs(stats.zscore(means)) if len(means) > 2 else [0] * len(means)
            std_z_scores = np.abs(stats.zscore(stds)) if len(stds) > 2 else [0] * len(stds)
            
            for i, batch_name in enumerate(batch_names):
                # ç»¼åˆè¯„åˆ†ï¼šå‡å€¼åç¦» + ç¨³å®šæ€§åç¦»
                outlier_score = mean_z_scores[i] + std_z_scores[i]
                batch_outlier_scores[batch_name] += outlier_score
            
            outlier_analysis[column] = {
                'batch_means': dict(zip(batch_names, means)),
                'batch_stds': dict(zip(batch_names, stds)),
                'mean_z_scores': dict(zip(batch_names, mean_z_scores)),
                'std_z_scores': dict(zip(batch_names, std_z_scores))
            }
    
    # æ’åºæ‰¾å‡ºæœ€å¼‚å¸¸çš„æ‰¹æ¬¡
    sorted_batches = sorted(batch_outlier_scores.items(), key=lambda x: x[1], reverse=True)
    
    return {
        'analysis_type': 'multi_batch_outlier_detection',
        'batch_outlier_scores': batch_outlier_scores,
        'outlier_details': outlier_analysis,
        'ranked_batches': sorted_batches,
        'most_abnormal_batches': sorted_batches[:3],  # å‰3ä¸ªæœ€å¼‚å¸¸çš„æ‰¹æ¬¡
        'summary': {
            'total_batches': len(batch_data),
            'analyzed_metrics': len(outlier_analysis),
            'most_abnormal': sorted_batches[0][0] if sorted_batches else None,
            'most_normal': sorted_batches[-1][0] if sorted_batches else None
        }
    }

def _analyze_multi_batch_comprehensive(analyzer, batch_data, columns, time_column):
    """ç»¼åˆåˆ†ææ‰€æœ‰æ‰¹æ¬¡"""
    comprehensive_results = {}
    
    # å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œç»¼åˆåˆ†æ
    for df, batch_name in batch_data:
        result = analyzer.comprehensive_analysis(df, columns, time_column)
        comprehensive_results[batch_name] = _simplify_comprehensive_analysis(result)
    
    # ç”Ÿæˆæ‰¹æ¬¡é—´å¯¹æ¯”æ‘˜è¦
    comparison_summary = _generate_multi_batch_summary(comprehensive_results)
    
    return {
        'analysis_type': 'multi_batch_comprehensive',
        'batch_results': comprehensive_results,
        'comparison_summary': comparison_summary,
        'total_batches': len(batch_data)
    }

def _analyze_time_series(analyzer, df, columns, time_column):
    """æ—¶é—´åºåˆ—åˆ†æï¼Œç”¨äºå•æ‰¹æ¬¡å˜åŒ–æ›²çº¿"""
    import numpy as np
    import pandas as pd
    
    print(f"ğŸ”§ [DEBUG] _analyze_time_series å¼€å§‹")
    print(f"ğŸ”§ [DEBUG] è¾“å…¥å‚æ•° - columns: {columns}, time_column: {time_column}")
    print(f"ğŸ”§ [DEBUG] æ•°æ®å½¢çŠ¶: {df.shape}, åˆ—å: {list(df.columns)}")
    
    if not time_column or time_column not in df.columns:
        # å¦‚æœæ²¡æœ‰æ—¶é—´åˆ—ï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºæ—¶é—´
        print("ğŸ”§ [DEBUG] æ²¡æœ‰æ—¶é—´åˆ—ï¼Œåˆ›å»ºtime_index")
        df = df.copy()
        df['time_index'] = range(len(df))
        time_column = 'time_index'
        print(f"ğŸ”§ [DEBUG] æ—¶é—´åˆ—è®¾ä¸º: {time_column}")
    
    # é€‰æ‹©è¦åˆ†æçš„åˆ—
    if columns:
        # ç”¨æˆ·æŒ‡å®šçš„åˆ—ï¼Œå°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹
        target_columns = []
        for col in columns:
            if col in df.columns and col != time_column:
                try:
                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹
                    pd.to_numeric(df[col], errors='coerce')
                    target_columns.append(col)
                except:
                    continue
    else:
        # è‡ªåŠ¨é€‰æ‹©æ•°å€¼å‹åˆ—ï¼Œæ’é™¤æ—¶é—´åˆ—
        target_columns = []
        for col in df.columns:
            if col != time_column:
                if df[col].dtype in ['int64', 'float64', 'float32', 'int32']:
                    target_columns.append(col)
                else:
                    # å°è¯•è½¬æ¢objectç±»å‹çš„åˆ—
                    try:
                        numeric_series = pd.to_numeric(df[col], errors='coerce')
                        if not numeric_series.isna().all():  # å¦‚æœä¸æ˜¯å…¨éƒ¨ä¸ºNaN
                            target_columns.append(col)
                    except:
                        continue
        target_columns = target_columns[:10]
    
    print(f"ğŸ”§ [DEBUG] æœ€ç»ˆç›®æ ‡åˆ—: {target_columns}")
    
    time_series_analysis = {}
    
    for column in target_columns:
        print(f"ğŸ”§ [DEBUG] å¤„ç†åˆ—: {column}")
        if column != time_column:
            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹å¹¶å»é™¤NaN
            try:
                numeric_values = pd.to_numeric(df[column], errors='coerce')
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶DataFrameç¡®ä¿ç´¢å¼•å¯¹åº”
                temp_df = pd.DataFrame({
                    'values': numeric_values,
                    'time': df[time_column]
                }).dropna()
                values = temp_df['values']
                time_values = temp_df['time']
            except:
                # ä½¿ç”¨åŸå§‹æ•°æ®
                temp_df = pd.DataFrame({
                    'values': df[column],
                    'time': df[time_column]
                }).dropna()
                values = temp_df['values']
                time_values = temp_df['time']
            
            if len(values) >= 2:
                print(f"ğŸ”§ [DEBUG] åˆ— {column} æœ‰ {len(values)} ä¸ªæœ‰æ•ˆæ•°æ®ç‚¹")
                # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
                mean_val = values.mean()
                std_val = values.std()
                
                # è®¡ç®—å˜åŒ–ç‡
                changes = values.diff().dropna()
                max_change = changes.abs().max() if len(changes) > 0 else 0
                
                # è¶‹åŠ¿åˆ†æ
                x = np.arange(len(values))
                if len(values) > 1:
                    slope = np.polyfit(x, values, 1)[0]
                    correlation = np.corrcoef(x, values)[0, 1] if len(values) > 2 else 0
                else:
                    slope = 0
                    correlation = 0
                
                time_series_analysis[column] = {
                    'values': values.tolist()[:100],  # é™åˆ¶æ•°æ®ç‚¹æ•°é‡
                    'time_points': time_values.tolist()[:100],
                    'mean': round(mean_val, 4),
                    'std': round(std_val, 4),
                    'max_change': round(max_change, 4),
                    'trend_slope': round(slope, 6),
                    'correlation': round(correlation, 4),
                    'trend_direction': 'ä¸Šå‡' if slope > 0.001 else ('ä¸‹é™' if slope < -0.001 else 'ç¨³å®š'),
                    'data_points': len(values)
                }
    
    return {
        'analysis_type': 'time_series',
        'time_column': time_column,
        'series_analysis': time_series_analysis,
        'total_data_points': len(df),
        'debug_info': {
            'target_columns': target_columns,
            'df_columns': list(df.columns),
            'df_dtypes': dict(df.dtypes.astype(str)),
            'analysis_count': len(time_series_analysis),
            'df_shape': df.shape,
            'specified_columns': columns
        }
    }

def _generate_multi_batch_summary(comprehensive_results):
    """ç”Ÿæˆå¤šæ‰¹æ¬¡å¯¹æ¯”æ‘˜è¦"""
    import numpy as np
    
    batch_names = list(comprehensive_results.keys())
    
    # ç»Ÿè®¡å„æ‰¹æ¬¡çš„å…³é”®æŒ‡æ ‡
    summary = {
        'best_stability_batch': None,
        'worst_stability_batch': None,
        'most_outliers_batch': None,
        'least_outliers_batch': None,
        'batch_rankings': {}
    }
    
    # ç®€å•çš„æ’åé€»è¾‘ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
    stability_scores = {}
    outlier_counts = {}
    
    for batch_name, result in comprehensive_results.items():
        # è®¡ç®—ç¨³å®šæ€§ç»¼åˆè¯„åˆ†
        stability_data = result.get('key_metrics_stability', {})
        if stability_data:
            avg_stability = np.mean([data.get('stability_index', 0) for data in stability_data.values()])
            stability_scores[batch_name] = avg_stability
        
        # è®¡ç®—å¼‚å¸¸å€¼æ€»æ•°
        outlier_data = result.get('outlier_summary', {})
        outlier_counts[batch_name] = outlier_data.get('total_outliers', 0)
    
    if stability_scores:
        summary['best_stability_batch'] = max(stability_scores, key=stability_scores.get)
        summary['worst_stability_batch'] = min(stability_scores, key=stability_scores.get)
    
    if outlier_counts:
        summary['most_outliers_batch'] = max(outlier_counts, key=outlier_counts.get)
        summary['least_outliers_batch'] = min(outlier_counts, key=outlier_counts.get)
    
    return summary
